\addcontentsline{toc}{section}{چکیده انگلیسی}
\thispagestyle{empty}

\begin{latin}
\begin{center}

{\huge
Smart Swapping Approach Based on Neural Network Models Similarity for Enhancing Federated Learning
}

\vspace{1cm}

{\LARGE{Ali Bozorgzad}}

\vspace{0.2cm}

{\small a.bozorgzad@ec.iut.ac.ir}

\vspace{0.5cm}

September 07, 2024

\vspace{0.5cm}

Department of Electrical and Computer Engineering

\vspace{0.2cm}

Isfahan University of Technology, Isfahan 84156-83111, Iran

\vspace{0.2cm}

Degree: M.Sc. \hspace*{3cm} Language: Farsi

\vspace{1cm}

{\small\textbf{Supervisor: Dr. Amir Khorsandi (khorsandi@iut.ac.ir)}}
\end{center}
~\vfill



\noindent\textbf{Abstract}

\begin{small}
\baselineskip=0.6cm
Today, the explosion of data generated due to the rapid advancement of technology and the increasing number of devices connected to the internet, is the main reason to grow artificial intelligence and machine learning. But this makes effective communication and also user privacy protection more crucial than ever. Distributed methods like federated learning, are considered as an appropriate solution to meet these requirements. In federated learning, data remains on local devices instead of being sent to a central server, and models are trained locally. Then, these models are combined to create a comprehensive model. This approach not only reduces the need to transfer data but also paves the way for better user privacy protection. However, federated learning faces numerous challenges such as the statistical heterogeneity of data. The data on different devices vary significantly and this can prevent local models from effectively learning all data features. Thus, the global model does not converge as expected. This necessitates methods that focus on solving this problem with moderated computational efficiency, communication bandwidth and privacy preservation. This can be achieved by swapping neural network models between end users during the learning process. In this manner, local models encounter more diverse data, and so the convergence of the global model can be improved. In conventional methods, swapping is done based on a random selection strategy. But this study suggests an intelligent swapping method based on the similarity criteria. In this method, models with the least similarity are swapped to expose unfamiliar data to models and enhance the convergence of the global model. Furthermore, the impact of this method on preserving user privacy is discussed. By using a central server as an intermediary in the swapping process, it is shown that the proposed method can simplify the implementation of various privacy techniques. The simulation results demonstrate that the proposed intelligent swapping method can accelerate the convergence of the global model by approximately 1{\footnotesize \(\%\)} in scenarios with a large number of users.



\end{small}

\vspace{0.5 cm}

\noindent \textbf{Key Words}: Federated Learning, Distributed Learning, Non-IID Data, Neural Network Similarity, Statistical Heterogeneity

\end{latin}